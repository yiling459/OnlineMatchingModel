{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Reward: 49\n",
      "OPT Reward: 49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class OnlineMatchingModel:\n",
    "    def __init__(self, N, T, E, rewards, usage_duration_distributions):\n",
    "        # Initialize the model parameters\n",
    "        self.N = N  # Number of offline vertices (resources)\n",
    "        self.T = T  # Number of online vertices (requests)\n",
    "        self.E = E  # Edges in the bipartite graph\n",
    "        self.rewards = rewards  # Reward for each resource\n",
    "        self.usage_duration_distributions = usage_duration_distributions  # Usage duration distributions\n",
    "\n",
    "    def greedy_algorithm(self):\n",
    "        total_reward = 0\n",
    "        resource_availability = [0] * self.N  # Track when resources become available\n",
    "        for t in range(self.T):\n",
    "            available_resources = [i for i in range(self.N) if t >= resource_availability[i]]\n",
    "            # Find the highest reward resource that can be matched\n",
    "            best_resource = None\n",
    "            best_reward = 0\n",
    "            for i in available_resources:\n",
    "                if (i, t) in self.E and self.rewards[i] > best_reward:\n",
    "                    best_reward = self.rewards[i]\n",
    "                    best_resource = i\n",
    "            if best_resource is not None:\n",
    "                total_reward += best_reward\n",
    "                # Sample the usage duration\n",
    "                duration = np.random.choice(self.usage_duration_distributions[best_resource])\n",
    "                resource_availability[best_resource] = t + duration\n",
    "        return total_reward\n",
    "\n",
    "    def clairvoyant_opt(self, arrival_sequence):\n",
    "        total_reward = 0\n",
    "        resource_availability = [0] * self.N\n",
    "        for t in range(self.T):\n",
    "            current_request = arrival_sequence[t]\n",
    "            # Check all resources that can be matched with the current request\n",
    "            for i in range(self.N):\n",
    "                if (i, current_request) in self.E and resource_availability[i] <= t:\n",
    "                    total_reward += self.rewards[i]\n",
    "                    duration = np.random.choice(self.usage_duration_distributions[i])\n",
    "                    resource_availability[i] = t + duration\n",
    "                    break  # Assuming one match per request\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "# Example usage\n",
    "N = 5  # Number of resources\n",
    "T = 10  # Number of requests\n",
    "E = [(i, j) for i in range(N) for j in range(T) if random.random() > 0.5]  # Randomly generate edges\n",
    "rewards = np.random.randint(1, 10, N)\n",
    "usage_duration_distributions = [np.random.randint(1, 5, 10) for _ in range(N)]\n",
    "\n",
    "model = OnlineMatchingModel(N, T, E, rewards, usage_duration_distributions)\n",
    "greedy_reward = model.greedy_algorithm()\n",
    "arrival_sequence = np.random.permutation(T)  # Random arrival sequence for OPT\n",
    "opt_reward = model.clairvoyant_opt(arrival_sequence)\n",
    "\n",
    "print(f\"Greedy Reward: {greedy_reward}\")\n",
    "print(f\"OPT Reward: {opt_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main differences between Inventory Balancing Algorithm and Rank Based Allocation Algorithm:\n",
    "* Goals:\n",
    "    * IBA: Balance the cost of holding inventory with the risk of being out of stock, optimizing inventory levels by adjusting replenishment points and order volumes.\n",
    "    * RBA: Maximize long-term rewards or utility while considering the reuse of resources. By prioritizing the highest-ranking available units, the algorithm attempts to optimize allocation decisions while keeping resources efficiently recycled.\n",
    "* Scenario:\n",
    "    * IBA: Inventory management and demand matching\n",
    "    * RBA: Address the reusability of resources and dynamically reprioritize resource units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inventory Balancing Algorithm (with unreusable resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 1, 4, 2, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "class OnlineMatchingModel:\n",
    "    def __init__(self, N, T, E, rewards):\n",
    "        self.N = N  # Number of offline vertices (resources)\n",
    "        self.T = T  # Number of online vertices (requests)\n",
    "        self.E = E  # Edges in the bipartite graph\n",
    "        self.rewards = rewards  # Reward for each resource\n",
    "        \n",
    "        self.inventory = {}\n",
    "        for n in range(N):\n",
    "            self.inventory[n] = 1 # Each resource is initially set to available\n",
    "\n",
    "        \n",
    "    def g(self, x):\n",
    "        return np.exp(-x)\n",
    "    # Used to adjust the selected weight or priority of each resource\n",
    "    # When the ratio of the resource's remaining inventory x is high, the value of e^(-x) is smaller, \n",
    "    # which means that the priority of resources being selected is correspondingly higher, \n",
    "    # because we are more inclined to use resources with more remaining inventory\n",
    "    \n",
    "    def allocate_resource(self, t):\n",
    "        scores = {}\n",
    "        for n in self.E[t]:\n",
    "            if self.inventory[n] > 0:  # Allocation is considered only when resources are available\n",
    "                score = (1 - self.g(self.inventory[n])) * self.rewards[n]\n",
    "                scores[n] = score\n",
    "\n",
    "        if scores:\n",
    "            selected_resource = max(scores, key=scores.get)\n",
    "            self.inventory[selected_resource] = 0  # After allocation, the resource is marked as unavailable\n",
    "            return selected_resource\n",
    "        return None  # If no resources are available, return None\n",
    "    \n",
    "# sample\n",
    "N = 5\n",
    "T = 10\n",
    "E = {}\n",
    "for t in range(T):\n",
    "    E[t] = range(N)\n",
    "\n",
    "rewards = np.random.rand(N)\n",
    "\n",
    "model = OnlineMatchingModel(N, T, E, rewards)\n",
    "\n",
    "allocations = []\n",
    "for t in range(T):\n",
    "    allocation = model.allocate_resource(t)\n",
    "    allocations.append(allocation)\n",
    "\n",
    "print(allocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inventory Balancing Algorithm (reusable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 3, 4, 4, 3, 4, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnlineMatchingModel:\n",
    "    def __init__(self, N, T, E, rewards, usage_duration_distributions):\n",
    "        self.N = N  # Number of offline vertices (resources)\n",
    "        self.T = T  # Number of online vertices (requests)\n",
    "        self.E = E  # Edges in the bipartite graph\n",
    "        self.rewards = rewards  # Reward for each resource\n",
    "        self.usage_duration_distributions = usage_duration_distributions  # Usage duration distributions\n",
    "\n",
    "        self.inventory = {}\n",
    "        for n in range(N):\n",
    "            self.inventory[n] = 1  # Each resource is initially set to available\n",
    "        \n",
    "        self.return_times = {}\n",
    "        for n in range(N):\n",
    "            self.return_times[n] = []  # Initializes the return time list for each resource\n",
    "    \n",
    "    def g(self, x):\n",
    "        return np.exp(-x)\n",
    "    # Used to adjust the selected weight or priority of each resource\n",
    "    # When the ratio of the resource's remaining inventory x is high, the value of e^(-x) is smaller, \n",
    "    # which means that the priority of resources being selected is correspondingly higher, \n",
    "    # because we are more inclined to use resources with more remaining inventory\n",
    "\n",
    "    \n",
    "    def update_inventory(self, current_time):\n",
    "        for n in range(self.N):\n",
    "            updated_return_times = []\n",
    "            for return_time in self.return_times[n]:\n",
    "                if return_time > current_time:\n",
    "                    updated_return_times.append(return_time)\n",
    "            self.return_times[n] = updated_return_times\n",
    "            \n",
    "            if not updated_return_times:  # If there is no resource waiting to be returned, it is set to available\n",
    "                self.inventory[n] = 1\n",
    "            else:\n",
    "                self.inventory[n] = 0\n",
    "    \n",
    "    def allocate_resource(self, t):\n",
    "        self.update_inventory(t)\n",
    "        \n",
    "        scores = {}\n",
    "        for n in self.E[t]:\n",
    "            score = (1 - self.g(self.inventory[n])) * self.rewards[n]\n",
    "            scores[n] = score\n",
    "        \n",
    "        selected_resource = max(scores, key=scores.get)\n",
    "        \n",
    "        duration = np.random.choice(self.usage_duration_distributions[selected_resource])\n",
    "        self.return_times[selected_resource].append(t + duration)\n",
    "        \n",
    "        self.inventory[selected_resource] = 0\n",
    "        \n",
    "        return selected_resource\n",
    "\n",
    "# sample\n",
    "N = 5\n",
    "T = 10\n",
    "E = {}\n",
    "for t in range(T):\n",
    "    E[t] = range(N)\n",
    "\n",
    "rewards = np.random.rand(N)\n",
    "\n",
    "usage_duration_distributions = {}\n",
    "for n in range(N):\n",
    "    usage_duration_distributions[n] = [1, 2, 3]\n",
    "\n",
    "model = OnlineMatchingModel(N, T, E, rewards, usage_duration_distributions)\n",
    "\n",
    "allocations = []\n",
    "for t in range(T):\n",
    "    allocation = model.allocate_resource(t)\n",
    "    allocations.append(allocation)\n",
    "\n",
    "print(allocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Based Allocation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage Duration Distributions: {0: [5, 4], 1: [4, 5], 2: [3, 4], 3: [5, 3]}\n",
      "At time 0, resource 1 unit 1 is allocated\n",
      "At time 1, resource 3 unit 1 is allocated\n",
      "At time 2, resource 2 unit 1 is allocated\n",
      "At time 3, resource 0 unit 1 is allocated\n",
      "At time 4, resource 0 unit 0 is allocated\n",
      "At time 5, resource 1 unit 1 is allocated\n",
      "At time 6, resource 3 unit 1 is allocated\n",
      "At time 7, resource 2 unit 1 is allocated\n",
      "At time 8, resource 0 unit 1 is allocated\n",
      "At time 9, resource 1 unit 1 is allocated\n",
      "At time 10, resource 3 unit 1 is allocated\n",
      "At time 11, resource 2 unit 1 is allocated\n",
      "At time 12, resource 0 unit 0 is allocated\n",
      "At time 13, resource 1 unit 1 is allocated\n",
      "At time 14, resource 3 unit 1 is allocated\n",
      "At time 15, resource 2 unit 1 is allocated\n",
      "At time 16, resource 0 unit 1 is allocated\n",
      "At time 17, resource 3 unit 1 is allocated\n",
      "At time 18, resource 1 unit 1 is allocated\n",
      "At time 19, resource 2 unit 1 is allocated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnlineMatchingModel:\n",
    "    def __init__(self, N, T, E, rewards, usage_duration_distributions):\n",
    "        self.N = N  # Number of resources\n",
    "        self.T = T  # Number of time steps or requests\n",
    "        self.E = E  # Edges representing possible matches between resources and requests\n",
    "        self.rewards = rewards  # Rewards for allocating each resource\n",
    "        self.usage_duration_distributions = usage_duration_distributions  # Usage durations for each resource\n",
    "        \n",
    "        # Initialize resource units' availability, rank, and return time\n",
    "        # self.unit_availability = {n: [True for _ in range(len(usage_duration_distributions[n]))] for n in range(N)}\n",
    "        # self.unit_rank = {n: list(range(len(usage_duration_distributions[n]))) for n in range(N)}\n",
    "        # self.unit_return_time = {n: [-1 for _ in range(len(usage_duration_distributions[n]))] for n in range(N)}\n",
    "        \n",
    "        # Initialize resource units' availability\n",
    "        # For each resource, create a list indicating whether each unit is available\n",
    "        self.unit_availability = {}\n",
    "        for n in range(self.N):  # Iterate over each resource\n",
    "            availability_list = []  # Initialize an empty list for storing availability status of each unit\n",
    "            for _ in range(len(self.usage_duration_distributions[n])):  # Iterate over the number of units for each resource\n",
    "                availability_list.append(True)  # Initially every unit is available\n",
    "            self.unit_availability[n] = availability_list  # Assign the list to the corresponding resource in the dictionary\n",
    "        \n",
    "        # Initialize resource units' rank\n",
    "        # For each resource, create a list indicating the rank of each unit\n",
    "        self.unit_rank = {}\n",
    "        for n in range(self.N):  # Iterate over each resource\n",
    "            rank_list = []  # Initialize an empty list for storing rank of each unit\n",
    "            for rank in range(len(self.usage_duration_distributions[n])):  # Iterate over the number of units for each resource\n",
    "                rank_list.append(rank)  # Assign a rank to each unit\n",
    "            self.unit_rank[n] = rank_list  # Assign the list to the corresponding resource in the dictionary\n",
    "        \n",
    "        # Initialize resource units' return time\n",
    "        # For each resource, create a list indicating the return time of each unit\n",
    "        self.unit_return_time = {}\n",
    "        for n in range(self.N):  # Iterate over each resource\n",
    "            return_time_list = []  # Initialize an empty list for storing return time of each unit\n",
    "            for _ in range(len(self.usage_duration_distributions[n])):  # Iterate over the number of units for each resource\n",
    "                return_time_list.append(-1)  # Initially, there is no return time (-1 indicates not in use)\n",
    "            self.unit_return_time[n] = return_time_list  # Assign the list to the corresponding resource in the dictionary\n",
    "\n",
    "\n",
    "    def g(self, x):\n",
    "        return np.exp(-x)\n",
    "    \n",
    "    def update_availability(self, t):\n",
    "        # Iterate over each resource to update its units' availability\n",
    "        for i in range(self.N):\n",
    "            # Iterate over each unit of the resource\n",
    "            for k in range(len(self.unit_availability[i])):\n",
    "                # Check if the current time is greater than or equal to the return time of the unit\n",
    "                if t >= self.unit_return_time[i][k]:\n",
    "                    # If so, make the unit available again\n",
    "                    self.unit_availability[i][k] = True\n",
    "\n",
    "    def allocate_resource(self, t):\n",
    "        # First, update the availability of all resources at time t\n",
    "        self.update_availability(t)\n",
    "        \n",
    "        # Initialize a dictionary to store the scores for each available resource\n",
    "        scores = {}\n",
    "        # Iterate over the resources that are available at time t\n",
    "        for i in self.E[t]:\n",
    "            # Find all available units of resource i\n",
    "            available_units = [k for k, available in enumerate(self.unit_availability[i]) if available]\n",
    "            # If there are available units, calculate the score for the resource\n",
    "            if available_units:\n",
    "                # Find the highest ranked available unit of resource i\n",
    "                highest_ranked_unit = max(available_units, key=lambda x: self.unit_rank[i][x])\n",
    "                # Calculate the score based on the reward and the rank of the unit\n",
    "                scores[i] = self.rewards[i] * (1 - self.g(self.unit_rank[i][highest_ranked_unit] / len(self.unit_rank[i])))\n",
    "\n",
    "        # If there are any scores calculated, proceed to allocate a resource\n",
    "        if scores:\n",
    "            # Select the resource with the highest score\n",
    "            selected_resource = max(scores, key=scores.get)\n",
    "            # Find the highest ranked available unit of the selected resource\n",
    "            selected_unit = max([k for k, available in enumerate(self.unit_availability[selected_resource]) if available], \n",
    "                                key=lambda x: self.unit_rank[selected_resource][x])\n",
    "            # Mark the selected unit as unavailable\n",
    "            self.unit_availability[selected_resource][selected_unit] = False\n",
    "            # Set the return time for the unit based on the selected usage duration\n",
    "            duration = np.random.choice(self.usage_duration_distributions[selected_resource])\n",
    "            self.unit_return_time[selected_resource][selected_unit] = t + duration\n",
    "            # Return the selected resource and unit\n",
    "            return selected_resource, selected_unit\n",
    "        else:\n",
    "            # If no resources are available, return None\n",
    "            return None, None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "N = 4  # Number of resources\n",
    "T = 20  # Number of time steps or requests\n",
    "E = {t: range(N) for t in range(T)}  # Possible matches between resources and requests\n",
    "rewards = np.random.rand(N)  # Random rewards for each resource\n",
    "\n",
    "# Generate usage duration distributions randomly\n",
    "usage_duration_distributions = {}\n",
    "for n in range(N):\n",
    "    usage_duration_distributions[n] = np.random.choice(range(3, 6), size=2, replace=False).tolist()\n",
    "\n",
    "print(\"Usage Duration Distributions:\", usage_duration_distributions)\n",
    "\n",
    "model = OnlineMatchingModel(N, T, E, rewards, usage_duration_distributions)\n",
    "\n",
    "for t in range(T):\n",
    "    resource, unit = model.allocate_resource(t)\n",
    "    if resource is not None:\n",
    "        print(f\"At time {t}, resource {resource} unit {unit} is allocated\")\n",
    "    else:\n",
    "        print(f\"At time {t}, no resource is allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Assortment Optimization for Reusable Products with Random Usage Durations\n",
    "\n",
    "It is assumed that the expected revenue of each product depends not only on its own attributes but also on time. To simplify, we assume that the expected return on each product decreases linearly over time, meaning that each product may become less attractive over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the Online Matching Model...\n",
      "\n",
      "Time Step 0:\n",
      "Allocations: [0]\n",
      "State:\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "Time Step 1:\n",
      "Allocations: [1]\n",
      "State:\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "Time Step 2:\n",
      "Allocations: [2]\n",
      "State:\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "\n",
      "Time Step 3:\n",
      "Allocations: [0]\n",
      "State:\n",
      "[[1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "\n",
      "Time Step 4:\n",
      "Allocations: [0]\n",
      "State:\n",
      "[[1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnlineMatchingModel:\n",
    "    def __init__(self, N, T, E, rewards, usage_duration_distributions):\n",
    "        self.N = N  # Number of resources\n",
    "        self.T = T  # Number of time steps or requests\n",
    "        self.E = E  # Edges representing possible matches between resources and requests\n",
    "        self.rewards = rewards  # Rewards for allocating each resource\n",
    "        self.usage_duration_distributions = usage_duration_distributions  # Usage durations for each resource\n",
    "        self.state = np.zeros((N, T))  # State matrix\n",
    "        self.linear_approximations = np.zeros((N, T))  # Linear approximation matrix\n",
    "\n",
    "    def compute_linear_approximations(self):\n",
    "        \"\"\"\n",
    "        Computes the linear approximation values for each resource at each time step.\n",
    "        \"\"\"\n",
    "        # Example: linearly decreasing rewards over time\n",
    "        for t in range(self.T):\n",
    "            for n in range(self.N):\n",
    "                self.linear_approximations[n, t] = self.rewards[n, t] - t * 0.1  # Example of a linear decrease\n",
    "\n",
    "    def transition_function(self, allocations, t):\n",
    "        \"\"\"\n",
    "        Update the state based on the allocations.\n",
    "        :param allocations: A list of resource indices that are allocated at time t.\n",
    "        :param t: The current time step.\n",
    "        \"\"\"\n",
    "        for n in allocations:\n",
    "            # Example: Mark the resource as used in the next time step based on usage duration\n",
    "            duration = np.random.choice(self.usage_duration_distributions[n])  # Randomly pick a duration\n",
    "            end_time = min(t + duration, self.T - 1)\n",
    "            self.state[n, t:end_time] = 1  # Mark as used\n",
    "\n",
    "    def greedy_policy(self, t):\n",
    "        \"\"\"\n",
    "        Selects resources to allocate based on the linear approximation values.\n",
    "        :param t: The current time step.\n",
    "        :return: A list of resources to allocate.\n",
    "        \"\"\"\n",
    "        available_resources = [n for n in range(self.N) if self.state[n, t] == 0]\n",
    "        if not available_resources:\n",
    "            return []\n",
    "        selected_resource = max(available_resources, key=lambda n: self.linear_approximations[n, t])\n",
    "        return [selected_resource]\n",
    "    \n",
    "    def run(self):\n",
    "        for t in range(self.T):\n",
    "            print(f\"\\nTime Step {t}:\")\n",
    "            allocations = self.greedy_policy(t)\n",
    "            print(f\"Allocations: {allocations}\")\n",
    "            self.transition_function(allocations, t)\n",
    "            print(f\"State:\\n{self.state}\")\n",
    "\n",
    "# Example usage\n",
    "N = 3  # Number of resources\n",
    "T = 5  # Number of time steps\n",
    "E = []  # Assuming E is not used in this example\n",
    "rewards = np.random.rand(N, T)  # Random rewards\n",
    "usage_duration_distributions = [[1, 2, 3], [2, 3, 4], [1, 3, 5]]  # Example usage durations\n",
    "\n",
    "model = OnlineMatchingModel(N, T, E, rewards, usage_duration_distributions)\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the Online Matching Model...\n",
      "\n",
      "Time Step 0: X---\n",
      "Time Step 1: XX--\n",
      "Time Step 2: XX-X\n",
      "Time Step 3: -XXX\n",
      "Time Step 4: X--X\n",
      "Time Step 5: -X-X\n",
      "Time Step 6: -XXX\n",
      "Time Step 7: XXX-\n",
      "Time Step 8: XXX-\n",
      "Time Step 9: X---\n",
      "Time Step 10: XX--\n",
      "Time Step 11: -XX-\n",
      "Time Step 12: XXX-\n",
      "Time Step 13: XX--\n",
      "Time Step 14: XX--\n",
      "Time Step 15: XXX-\n",
      "Time Step 16: -XX-\n",
      "Time Step 17: -XX-\n",
      "Time Step 18: XXX-\n",
      "Time Step 19: XXX-\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OnlineMatchingModel:\n",
    "    def __init__(self, N, T, E, rewards, usage_duration_distributions):\n",
    "        self.N = N  # Number of resources\n",
    "        self.T = T  # Number of time steps or requests\n",
    "        self.E = E  # Edges (not utilized in this simplified example)\n",
    "        self.rewards = rewards  # Matrix of rewards\n",
    "        self.usage_duration_distributions = usage_duration_distributions  # Distribution of usage durations\n",
    "        self.state = np.zeros((N, T))  # State of each resource over time\n",
    "        self.linear_approximations = np.zeros((N, T))  # Linear approximation values\n",
    "\n",
    "    def compute_linear_approximations(self):\n",
    "        # Compute linear approximation values\n",
    "        for t in range(self.T):\n",
    "            for n in range(self.N):\n",
    "                self.linear_approximations[n, t] = self.rewards[n, t] - t * 0.1\n",
    "\n",
    "    def transition_function(self, allocations, t):\n",
    "        # Update resource states based on allocations\n",
    "        for n in allocations:\n",
    "            duration = np.random.choice(self.usage_duration_distributions[n])\n",
    "            end_time = min(t + duration, self.T)\n",
    "            self.state[n, t:end_time] = 1  # Mark as used\n",
    "\n",
    "    def greedy_policy(self, t):\n",
    "        # Select resource with the highest approximation value\n",
    "        available_resources = [n for n in range(self.N) if self.state[n, t] == 0]\n",
    "        if not available_resources:\n",
    "            return []\n",
    "        return [max(available_resources, key=lambda n: self.linear_approximations[n, t])]\n",
    "\n",
    "    def run(self):\n",
    "        # Execute the model\n",
    "        self.compute_linear_approximations()\n",
    "        print(\"Running the Online Matching Model...\\n\")\n",
    "        for t in range(self.T):\n",
    "            allocations = self.greedy_policy(t)\n",
    "            self.transition_function(allocations, t)\n",
    "            # Print resource states for this time step\n",
    "            resource_states = ''.join(['X' if self.state[n, t] else '-' for n in range(self.N)])\n",
    "            print(f\"Time Step {t}: {resource_states}\")\n",
    "\n",
    "# Example usage\n",
    "N = 4  # Number of resources\n",
    "T = 20  # Number of time steps\n",
    "E = []  # Edges (not utilized in this example)\n",
    "rewards = np.random.rand(N, T)  # Random rewards\n",
    "usage_duration_distributions = [[1, 2, 3], [2, 3, 4], [1, 2, 4], [2, 3, 5]]  # Example usage durations\n",
    "\n",
    "model = OnlineMatchingModel(N, T, E, rewards, usage_duration_distributions)\n",
    "model.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
